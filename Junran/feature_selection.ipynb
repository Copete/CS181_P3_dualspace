{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "labels = np.array(['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunksize = 500\n",
    "# n_chunks = 0\n",
    "\n",
    "# X_train = np.empty((0, 193))\n",
    "# y_train = np.empty((0, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk # 1\n",
      "(500L, 40L) (500L, 12L) (500L, 7L) (500L, 128L) (500L, 6L) (500L, 193L)\n",
      "500 samples\n",
      "Chunk # 2\n",
      "(500L, 40L) (500L, 12L) (500L, 7L) (500L, 128L) (500L, 6L) (500L, 193L)\n",
      "1000 samples\n",
      "X_train_mfcc.shape (1000L, 193L) y_train.shape (1000L, 1L)\n"
     ]
    }
   ],
   "source": [
    "# mfcc0 40, chroma 12, mel 7, contrast 128, tonnetz 6\n",
    "for df in pd.read_csv('./train.csv', header=None, chunksize=chunksize):\n",
    "    n_chunks += 1\n",
    "    print 'Chunk #', n_chunks\n",
    "    \n",
    "    X_train0 = df.values[:, :-1]\n",
    "    y_train0 = df.values[:, -1].reshape(-1, 1).astype(int)\n",
    "    \n",
    "    mfcc0 = np.array([np.mean(librosa.feature.mfcc(y=x, sr=SAMPLE_RATE, n_mfcc=40).T, axis=0) for x in X_train0])\n",
    "    chroma = np.array([np.mean(librosa.feature.chroma_stft(S=np.abs(librosa.stft(x)), sr=SAMPLE_RATE).T, axis=0) for x in X_train0])\n",
    "    contrast = np.array([np.mean(librosa.feature.spectral_contrast(S=np.abs(librosa.stft(x)), sr=SAMPLE_RATE).T, axis=0) for x in X_train0])\n",
    "    mel = np.array([np.mean(librosa.feature.melspectrogram(x, sr=SAMPLE_RATE).T, axis=0) for x in X_train0])\n",
    "    tonnetz = np.array([np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(x), sr=SAMPLE_RATE).T, axis=0) for x in X_train0])\n",
    "    feature0 = np.hstack([mfcc0, chroma, mel, contrast, tonnetz])\n",
    "    print mfcc0.shape, chroma.shape, contrast.shape, mel.shape, tonnetz.shape, feature0.shape\n",
    "    \n",
    "    X_train = np.vstack([X_train, feature0])\n",
    "    y_train =  y_train0 if n_chunks == 1 else np.vstack([y_train, y_train0])\n",
    "    print X_train.shape[0], 'samples'\n",
    "    if n_chunks > 1:\n",
    "        break\n",
    "    \n",
    "print 'X_train_mfcc.shape', X_train.shape, 'y_train.shape', y_train.shape\n",
    "N_train = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mfcc = X_train[:, :40]\n",
    "X_train_chroma = X_train[:, 40: 52]\n",
    "X_train_mel = X_train[:, 52: 59]\n",
    "X_train_contrast = X_train[:, 59: 187]\n",
    "X_train_tonnetz = X_train[:, 187:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 10\n",
    "N_ESTIMATORS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=10, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestClassifier(class_weight='balanced', n_jobs=-1, max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS)\n",
    "RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 8, 4, 5, 0, 4, 9, 9, 0, 9, 2, 9, 0, 8, 5, 0, 9, 7, 2, 8, 0, 7,\n",
       "       9, 5, 9, 0, 7, 0, 2, 9, 8, 9, 5, 0, 2, 5, 8, 9, 2, 9, 7, 7, 9, 0,\n",
       "       3, 0, 7, 7, 1, 0, 5, 0, 2, 9, 2, 8, 7, 0, 7, 2, 5, 8, 0, 4, 9, 2,\n",
       "       4, 4, 0, 7, 9, 0, 5, 9, 2, 4, 0, 0, 4, 0, 7, 7, 0, 7, 9, 0, 0, 2,\n",
       "       9, 8, 4, 7, 5, 7, 5, 9, 8, 8, 9, 0, 7, 7, 0, 1, 7, 4, 0, 0, 9, 7,\n",
       "       7, 7, 4, 7, 0, 8, 9, 0, 3, 2, 0, 9, 7, 9, 7, 9, 0, 2, 9, 5, 3, 7,\n",
       "       0, 5, 0, 1, 7, 4, 9, 5, 3, 0, 9, 3, 1, 9, 9, 7, 8, 8, 7, 4, 8, 0,\n",
       "       5, 8, 9, 2, 2, 4, 0, 5, 0, 8, 3, 8, 0, 9, 0, 4, 8, 0, 9, 0, 5, 4,\n",
       "       2, 3, 2, 5, 0, 2, 8, 2, 7, 7, 1, 0, 2, 2, 8, 9, 8, 7, 8, 8, 8, 4,\n",
       "       7, 9, 8, 3, 2, 8, 0, 9, 7, 0, 2, 5, 8, 3, 8, 5, 0, 2, 3, 0, 3, 2,\n",
       "       0, 3, 2, 5, 8, 0, 5, 4, 3, 8, 3, 9, 7, 8, 2, 9, 0, 0, 9, 5, 7, 9,\n",
       "       4, 0, 4, 2, 2, 2, 3, 0, 0, 0, 3, 2, 4, 7, 7, 7, 2, 8, 2, 7, 4, 9,\n",
       "       9, 4, 3, 0, 0, 2, 7, 7, 9, 8, 3, 9, 5, 0, 9, 5, 4, 9, 2, 3, 2, 4,\n",
       "       9, 8, 9, 9, 1, 8, 7, 0, 7, 0, 4, 9, 9, 7, 4, 3, 7, 9, 7, 0, 9, 0,\n",
       "       9, 2, 1, 5, 9, 1, 0, 8, 2, 2, 0, 7, 0, 3, 7, 2, 5, 3, 7, 8, 7, 8,\n",
       "       4, 4, 0, 7, 8, 3, 1, 9, 3, 9, 3, 9, 2, 0, 0, 8, 7, 4, 0, 8, 2, 2,\n",
       "       3, 5, 0, 8, 4, 3, 5, 7, 0, 6, 4, 0, 9, 8, 4, 3, 2, 4, 9, 9, 5, 4,\n",
       "       2, 0, 5, 9, 1, 7, 2, 9, 3, 0, 9, 5, 9, 4, 9, 4, 5, 7, 2, 8, 9, 0,\n",
       "       9, 7, 2, 5, 8, 0, 0, 8, 8, 0, 9, 0, 4, 2, 8, 7, 7, 7, 5, 3, 5, 2,\n",
       "       0, 3, 8, 7, 8, 7, 7, 3, 7, 2, 2, 7, 8, 9, 3, 4, 5, 3, 5, 5, 9, 0,\n",
       "       0, 9, 0, 5, 5, 7, 0, 4, 4, 4, 2, 8, 2, 9, 0, 5, 4, 0, 1, 2, 9, 4,\n",
       "       7, 4, 7, 2, 6, 7, 2, 0, 2, 8, 2, 1, 0, 7, 0, 4, 2, 5, 8, 2, 5, 0,\n",
       "       7, 0, 3, 8, 0, 6, 0, 4, 8, 7, 7, 8, 0, 9, 9, 3, 4, 2, 9, 7, 0, 4,\n",
       "       9, 9, 0, 0, 8, 1, 2, 8, 4, 5, 4, 2, 4, 8, 9, 8, 9, 8, 0, 0, 7, 0,\n",
       "       4, 0, 2, 8, 1, 2, 0, 9, 2, 5, 8, 7, 1, 5, 4, 3, 9, 2, 8, 8, 0, 4,\n",
       "       9, 3, 9, 7, 9, 7, 9, 0, 5, 9, 0, 3, 6, 2, 5, 8, 0, 7, 4, 9, 1, 9,\n",
       "       4, 9, 2, 4, 0, 1, 8, 5, 2, 5, 3, 9, 9, 8, 4, 3, 0, 4, 5, 0, 2, 4,\n",
       "       1, 0, 2, 8, 7, 2, 5, 7, 4, 7, 0, 5, 2, 2, 8, 4, 7, 0, 2, 3, 2, 3,\n",
       "       2, 9, 0, 8, 0, 4, 4, 3, 0, 4, 5, 8, 7, 8, 0, 8, 0, 9, 0, 2, 9, 0,\n",
       "       9, 4, 7, 1, 7, 2, 8, 0, 5, 2, 0, 9, 1, 8, 1, 9, 8, 9, 7, 0, 0, 0,\n",
       "       3, 5, 1, 2, 5, 0, 9, 4, 1, 4, 5, 9, 0, 9, 5, 9, 0, 5, 5, 4, 2, 2,\n",
       "       8, 9, 5, 2, 7, 5, 2, 0, 2, 0, 9, 9, 5, 3, 7, 7, 0, 4, 3, 2, 5, 5,\n",
       "       1, 0, 9, 5, 8, 7, 7, 1, 8, 2, 5, 2, 0, 4, 5, 0, 9, 0, 4, 8, 9, 3,\n",
       "       1, 9, 5, 9, 4, 7, 8, 3, 0, 5, 2, 8, 4, 1, 9, 4, 0, 8, 0, 3, 0, 1,\n",
       "       1, 7, 0, 4, 3, 5, 0, 4, 7, 0, 4, 9, 2, 9, 8, 0, 8, 4, 0, 4, 7, 2,\n",
       "       4, 7, 0, 9, 2, 0, 0, 4, 4, 9, 0, 0, 7, 2, 8, 1, 7, 7, 0, 9, 7, 0,\n",
       "       4, 0, 4, 8, 2, 2, 2, 8, 5, 0, 5, 7, 7, 3, 8, 4, 5, 0, 7, 4, 9, 9,\n",
       "       7, 8, 9, 2, 5, 4, 8, 4, 2, 0, 3, 3, 0, 4, 3, 1, 3, 8, 0, 0, 2, 3,\n",
       "       2, 8, 2, 3, 8, 2, 8, 8, 8, 0, 8, 4, 4, 4, 9, 0, 2, 2, 5, 0, 0, 8,\n",
       "       3, 7, 8, 3, 8, 4, 8, 5, 5, 4, 2, 4, 9, 3, 0, 2, 0, 0, 9, 9, 2, 3,\n",
       "       3, 9, 0, 2, 8, 0, 2, 2, 4, 5, 5, 8, 5, 4, 3, 3, 5, 2, 4, 8, 4, 9,\n",
       "       7, 0, 5, 5, 8, 3, 5, 7, 0, 4, 2, 0, 8, 4, 9, 5, 3, 5, 4, 5, 2, 2,\n",
       "       8, 0, 7, 8, 0, 7, 1, 5, 8, 0, 2, 4, 7, 0, 0, 7, 0, 0, 9, 3, 2, 9,\n",
       "       9, 3, 7, 5, 0, 9, 0, 2, 7, 2, 8, 0, 2, 0, 0, 5, 9, 2, 5, 9, 0, 1,\n",
       "       5, 7, 7, 3, 8, 4, 5, 1, 9, 3, 8, 8, 2, 7, 2, 9, 8, 9, 9, 9, 5, 5,\n",
       "       9, 8, 0, 0, 0, 7, 0, 1, 1, 9])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk # 1\n",
      "(200L, 88200L)\n",
      "(200L, 40L) (200L, 12L) (200L, 7L) (200L, 128L) (200L, 6L) (200L, 233L)\n",
      "200 samples\n",
      "Chunk # 2\n",
      "(200L, 88200L)\n",
      "(200L, 40L) (200L, 12L) (200L, 7L) (200L, 128L) (200L, 6L) (200L, 233L)\n",
      "400 samples\n",
      "Chunk # 3\n",
      "(200L, 88200L)\n",
      "(200L, 40L) (200L, 12L) (200L, 7L) (200L, 128L) (200L, 6L) (200L, 233L)\n",
      "600 samples\n",
      "Chunk # 4\n",
      "(200L, 88200L)\n",
      "(200L, 40L) (200L, 12L) (200L, 7L) (200L, 128L) (200L, 6L) (200L, 233L)\n",
      "800 samples\n",
      "Chunk # 5\n",
      "(200L, 88200L)\n",
      "(200L, 40L) (200L, 12L) (200L, 7L) (200L, 128L) (200L, 6L) (200L, 233L)\n",
      "1000 samples\n"
     ]
    }
   ],
   "source": [
    "chunksize = 200\n",
    "n_chunks = 0\n",
    "outputfile = open('x_test.csv.npy', 'a')\n",
    "\n",
    "X_test = np.empty((0, 233))\n",
    "# mfcc0 40, mfcc_std0 40, chroma 12, mel 7, contrast 128, tonnetz 6\n",
    "for df in pd.read_csv('./test.csv', header=None, chunksize=chunksize):\n",
    "    n_chunks += 1\n",
    "    print 'Chunk #', n_chunks\n",
    "    \n",
    "    X_test0 = df.values[:, 1:]\n",
    "    print X_test0.shape\n",
    "    \n",
    "    mfcc0 = np.array([np.mean(librosa.feature.mfcc(y=x, sr=SAMPLE_RATE, n_mfcc=40).T, axis=0) for x in X_test0])\n",
    "    mfcc_std0 = np.array([np.std(librosa.feature.mfcc(y=x, sr=SAMPLE_RATE, n_mfcc=40).T, axis=0) for x in X_test0])\n",
    "    chroma = np.array([np.mean(librosa.feature.chroma_stft(S=np.abs(librosa.stft(x)), sr=SAMPLE_RATE).T, axis=0) for x in X_test0])\n",
    "    contrast = np.array([np.mean(librosa.feature.spectral_contrast(S=np.abs(librosa.stft(x)), sr=SAMPLE_RATE).T, axis=0) for x in X_test0])\n",
    "    mel = np.array([np.mean(librosa.feature.melspectrogram(x, sr=SAMPLE_RATE).T, axis=0) for x in X_test0])\n",
    "    tonnetz = np.array([np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(x), sr=SAMPLE_RATE).T, axis=0) for x in X_test0])\n",
    "    feature0 = np.hstack([mfcc0, mfcc_std0, chroma, mel, contrast, tonnetz])\n",
    "    print mfcc0.shape, chroma.shape, contrast.shape, mel.shape, tonnetz.shape, feature0.shape\n",
    "    \n",
    "    X_test = np.vstack([X_test, feature0])\n",
    "    print X_test.shape[0], 'samples'\n",
    "    \n",
    "np.savetxt(outputfile, X_test)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000L, 233L)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('x_test.csv.npy', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000L, 233L)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 193)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-cf95923174da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    355\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    375\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    460\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 462\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 193)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "y_hat = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, predictions):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, p in enumerate(predictions):\n",
    "            f.write(str(i + 1) + ',' + str(p) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.vstack((range(len(preds)), preds)).T\n",
    "df = pd.DataFrame(result)\n",
    "df = df.astype(int)\n",
    "df.to_csv('sample_result.csv', index=False, header=['Id', 'Prediction'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
